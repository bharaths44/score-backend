{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bharaths/Developer/score/.venv/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_trf' (3.7.1) was trained with spaCy v3.7.0 and may not be 100% compatible with the current version (3.8.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "2025-03-13 23:02:48,758 - INFO - Initialized classifier with 0 existing topics\n",
      "2025-03-13 23:02:48,786 - INFO - Found 2 PDF files in /Users/bharaths/Developer/score/\n",
      "2025-03-13 23:02:48,792 - INFO - Processing document: /Users/bharaths/Developer/score/Module II Part -B.docx.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing directory: /Users/bharaths/Developer/score/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 23:02:52,984 - INFO - Extracted 10 keywords\n",
      "2025-03-13 23:02:52,996 - ERROR - Error generating topics: max_df corresponds to < documents than min_df\n",
      "2025-03-13 23:02:52,997 - INFO - Processing document: /Users/bharaths/Developer/score/temp_Module 1.pdf\n",
      "2025-03-13 23:02:55,772 - INFO - Extracted 10 keywords\n",
      "2025-03-13 23:02:55,776 - ERROR - Error generating topics: max_df corresponds to < documents than min_df\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Classification Summary =====\n",
      "\n",
      "File: Module II Part -B.docx.pdf\n",
      "Top Keywords: parsing, stack, input, reduce, precedence\n",
      "\n",
      "File: temp_Module 1.pdf\n",
      "Top Keywords: actions, the agent, example, artificial intelligence, knowledge\n",
      "\n",
      "===== Topic Hierarchy =====\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import PyPDF2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Initialize NLTK resources\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "    nltk.data.find(\"corpora/wordnet\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_trf\")\n",
    "except:\n",
    "    logging.info(\"Downloading spaCy model...\")\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "class DynamicTopicClassifier:\n",
    "    def __init__(self, topics_file=\"topics_database.json\"):\n",
    "        \"\"\"Initialize the dynamic topic classifier\"\"\"\n",
    "        self.stop_words = set(stopwords.words(\"english\"))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # For TF-IDF approach\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=5000, stop_words=\"english\", ngram_range=(1, 3), min_df=2\n",
    "        )\n",
    "\n",
    "        # For topic modeling\n",
    "        self.nmf_model = NMF(n_components=10, random_state=42)\n",
    "        self.lda_model = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "\n",
    "        # Topic database\n",
    "        self.topics_file = topics_file\n",
    "        self.topics_db = self.load_topics_database()\n",
    "\n",
    "        logging.info(\n",
    "            f\"Initialized classifier with {len(self.topics_db['topics'])} existing topics\"\n",
    "        )\n",
    "\n",
    "    def load_topics_database(self):\n",
    "        \"\"\"Load the topics database from file or create a new one\"\"\"\n",
    "        if os.path.exists(self.topics_file):\n",
    "            with open(self.topics_file, \"r\") as f:\n",
    "                return json.load(f)\n",
    "        else:\n",
    "            # Initialize with empty structure\n",
    "            topics_db = {\"topics\": {}, \"hierarchies\": {}, \"documents\": []}\n",
    "            return topics_db\n",
    "\n",
    "    def save_topics_database(self):\n",
    "        \"\"\"Save the topics database to file\"\"\"\n",
    "        with open(self.topics_file, \"w\") as f:\n",
    "            json.dump(self.topics_db, f, indent=2)\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and preprocess text\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove special characters\n",
    "        text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "        # Tokenize\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        # Remove stopwords and lemmatize\n",
    "        tokens = [\n",
    "            self.lemmatizer.lemmatize(word)\n",
    "            for word in tokens\n",
    "            if word not in self.stop_words and len(word) > 2\n",
    "        ]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text content from a PDF file\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            with open(pdf_path, \"rb\") as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                for page in reader.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \" \"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting text from PDF {pdf_path}: {str(e)}\")\n",
    "        return text\n",
    "\n",
    "    def extract_keywords(self, text, n=10):\n",
    "        \"\"\"Extract keywords from text using spaCy\"\"\"\n",
    "        try:\n",
    "            # Limit text length to avoid memory issues\n",
    "            doc = nlp(text[:20000])\n",
    "\n",
    "            # Extract noun phrases as potential keywords\n",
    "            keywords = []\n",
    "            for chunk in doc.noun_chunks:\n",
    "                if 1 <= len(chunk.text.split()) <= 3:  # Phrases with 1-3 words\n",
    "                    clean_text = re.sub(r\"[^\\w\\s]\", \"\", chunk.text.lower())\n",
    "                    if clean_text and len(clean_text) > 2:\n",
    "                        keywords.append(clean_text)\n",
    "\n",
    "            # Extract important entities\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ in [\n",
    "                    \"ORG\",\n",
    "                    \"PRODUCT\",\n",
    "                    \"WORK_OF_ART\",\n",
    "                    \"EVENT\",\n",
    "                    \"LAW\",\n",
    "                    \"LANGUAGE\",\n",
    "                ]:\n",
    "                    clean_text = re.sub(r\"[^\\w\\s]\", \"\", ent.text.lower())\n",
    "                    if clean_text and len(clean_text) > 2:\n",
    "                        keywords.append(clean_text)\n",
    "\n",
    "            # Count frequencies and return top n\n",
    "            keyword_freq = {}\n",
    "            for kw in keywords:\n",
    "                if kw not in self.stop_words and len(kw) > 2:\n",
    "                    keyword_freq[kw] = keyword_freq.get(kw, 0) + 1\n",
    "\n",
    "            sorted_keywords = sorted(\n",
    "                keyword_freq.items(), key=lambda x: x[1], reverse=True\n",
    "            )\n",
    "            return [kw for kw, _ in sorted_keywords[:n]]\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting keywords: {str(e)}\")\n",
    "            # Simple fallback - extract words by frequency\n",
    "            words = text.lower().split()\n",
    "            word_freq = {}\n",
    "            for word in words:\n",
    "                word = re.sub(r\"[^\\w]\", \"\", word)\n",
    "                if word and word not in self.stop_words and len(word) > 3:\n",
    "                    word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "            sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "            return [word for word, _ in sorted_words[:n]]\n",
    "\n",
    "    def generate_topics(self, text):\n",
    "        \"\"\"Generate topics using NMF and LDA\"\"\"\n",
    "        try:\n",
    "            # Vectorize the text\n",
    "            vectorized_text = self.vectorizer.fit_transform([text])\n",
    "\n",
    "            # NMF topic modeling\n",
    "            nmf_topics = self.nmf_model.fit_transform(vectorized_text)\n",
    "            nmf_components = self.nmf_model.components_\n",
    "\n",
    "            # LDA topic modeling\n",
    "            lda_topics = self.lda_model.fit_transform(vectorized_text)\n",
    "            lda_components = self.lda_model.components_\n",
    "\n",
    "            # Extract top words for each topic\n",
    "            feature_names = self.vectorizer.get_feature_names_out()\n",
    "\n",
    "            nmf_topic_words = []\n",
    "            for topic_idx, topic in enumerate(nmf_components):\n",
    "                top_features_ind = topic.argsort()[: -10 - 1 : -1]\n",
    "                top_features = [feature_names[i] for i in top_features_ind]\n",
    "                nmf_topic_words.append(top_features)\n",
    "\n",
    "            lda_topic_words = []\n",
    "            for topic_idx, topic in enumerate(lda_components):\n",
    "                top_features_ind = topic.argsort()[: -10 - 1 : -1]\n",
    "                top_features = [feature_names[i] for i in top_features_ind]\n",
    "                lda_topic_words.append(top_features)\n",
    "\n",
    "            return nmf_topic_words, lda_topic_words\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating topics: {str(e)}\")\n",
    "            return [], []\n",
    "\n",
    "    def find_similar_topics(self, keywords):\n",
    "        \"\"\"Find similar topics in the database based on keyword overlap\"\"\"\n",
    "        similar_topics = []\n",
    "\n",
    "        # Check keywords against existing topics\n",
    "        for topic, data in self.topics_db[\"topics\"].items():\n",
    "            # Check for keyword overlap\n",
    "            topic_keywords = data.get(\"keywords\", [])\n",
    "            overlap = set(keywords).intersection(set(topic_keywords))\n",
    "\n",
    "            if overlap:\n",
    "                similarity = len(overlap) / max(len(keywords), len(topic_keywords))\n",
    "                similar_topics.append((topic, similarity))\n",
    "\n",
    "        # Sort by similarity score\n",
    "        return sorted(similar_topics, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    def classify_document(self, file_path):\n",
    "        \"\"\"Classify a document and return potential topics\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            logging.error(f\"File not found: {file_path}\")\n",
    "            return {\"error\": \"File not found\"}\n",
    "\n",
    "        logging.info(f\"Processing document: {file_path}\")\n",
    "\n",
    "        # Extract text from PDF\n",
    "        text = self.extract_text_from_pdf(file_path)\n",
    "        if not text:\n",
    "            logging.error(f\"Could not extract text from: {file_path}\")\n",
    "            return {\"error\": \"Could not extract text from document\"}\n",
    "\n",
    "        # Preprocess text\n",
    "        processed_text = self.preprocess_text(text)\n",
    "\n",
    "        # Extract keywords\n",
    "        keywords = self.extract_keywords(text)\n",
    "        logging.info(f\"Extracted {len(keywords)} keywords\")\n",
    "\n",
    "        # Generate potential topics\n",
    "        nmf_topics, lda_topics = self.generate_topics(processed_text)\n",
    "\n",
    "        # Find similar existing topics\n",
    "        similar_topics = self.find_similar_topics(keywords)\n",
    "\n",
    "        # Prepare suggested topics\n",
    "        suggested_topics = []\n",
    "        for topic, score in similar_topics:\n",
    "            if score > 0.1:  # Minimum similarity threshold\n",
    "                suggested_topics.append(\n",
    "                    {\n",
    "                        \"name\": topic,\n",
    "                        \"similarity\": score,\n",
    "                        \"keywords\": self.topics_db[\"topics\"][topic][\"keywords\"][:5],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Generate potential new topics from NMF and LDA\n",
    "        potential_topics = []\n",
    "\n",
    "        # From NMF\n",
    "        for i, topic_words in enumerate(nmf_topics):\n",
    "            if topic_words:  # Skip empty topics\n",
    "                potential_topics.append(\n",
    "                    {\n",
    "                        \"name\": f\"Topic {i + 1}\",\n",
    "                        \"keywords\": topic_words[:5],\n",
    "                        \"source\": \"NMF\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # From LDA\n",
    "        for i, topic_words in enumerate(lda_topics):\n",
    "            if topic_words:  # Skip empty topics\n",
    "                potential_topics.append(\n",
    "                    {\n",
    "                        \"name\": f\"Topic {i + 1}\",\n",
    "                        \"keywords\": topic_words[:5],\n",
    "                        \"source\": \"LDA\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return {\n",
    "            \"file\": os.path.basename(file_path),\n",
    "            \"file_path\": file_path,\n",
    "            \"keywords\": keywords,\n",
    "            \"suggested_topics\": suggested_topics,\n",
    "            \"potential_topics\": potential_topics,\n",
    "            \"text_preview\": text[:500] + \"...\" if len(text) > 500 else text,\n",
    "        }\n",
    "\n",
    "    def add_topic(self, topic_name, keywords=None, parent_topic=None):\n",
    "        \"\"\"Add a new topic to the database\"\"\"\n",
    "        if topic_name in self.topics_db[\"topics\"]:\n",
    "            logging.warning(f\"Topic already exists: {topic_name}\")\n",
    "            return False\n",
    "\n",
    "        logging.info(f\"Adding new topic: {topic_name}\")\n",
    "        self.topics_db[\"topics\"][topic_name] = {\n",
    "            \"keywords\": keywords or [],\n",
    "            \"documents\": [],\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "        }\n",
    "\n",
    "        # Add hierarchy relationship if parent topic is provided\n",
    "        if parent_topic:\n",
    "            if parent_topic not in self.topics_db[\"hierarchies\"]:\n",
    "                self.topics_db[\"hierarchies\"][parent_topic] = []\n",
    "\n",
    "            if topic_name not in self.topics_db[\"hierarchies\"][parent_topic]:\n",
    "                self.topics_db[\"hierarchies\"][parent_topic].append(topic_name)\n",
    "\n",
    "        self.save_topics_database()\n",
    "        return True\n",
    "\n",
    "    def add_document_to_topic(self, doc_id, topic_name, file_path, keywords):\n",
    "        \"\"\"Add a document to a topic\"\"\"\n",
    "        # Ensure topic exists\n",
    "        if topic_name not in self.topics_db[\"topics\"]:\n",
    "            logging.warning(f\"Topic does not exist: {topic_name}\")\n",
    "            return False\n",
    "\n",
    "        # Create document entry if it doesn't exist\n",
    "        doc_exists = False\n",
    "        for doc in self.topics_db[\"documents\"]:\n",
    "            if doc[\"id\"] == doc_id:\n",
    "                doc_exists = True\n",
    "                if topic_name not in doc[\"topics\"]:\n",
    "                    doc[\"topics\"].append(topic_name)\n",
    "                break\n",
    "\n",
    "        if not doc_exists:\n",
    "            self.topics_db[\"documents\"].append(\n",
    "                {\n",
    "                    \"id\": doc_id,\n",
    "                    \"file_path\": file_path,\n",
    "                    \"file_name\": os.path.basename(file_path),\n",
    "                    \"topics\": [topic_name],\n",
    "                    \"keywords\": keywords,\n",
    "                    \"added_at\": datetime.now().isoformat(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Add document to topic\n",
    "        if doc_id not in self.topics_db[\"topics\"][topic_name][\"documents\"]:\n",
    "            self.topics_db[\"topics\"][topic_name][\"documents\"].append(doc_id)\n",
    "\n",
    "            # Update topic keywords based on document keywords\n",
    "            self.topics_db[\"topics\"][topic_name][\"keywords\"] = list(\n",
    "                set(self.topics_db[\"topics\"][topic_name][\"keywords\"]) | set(keywords)\n",
    "            )\n",
    "\n",
    "        self.save_topics_database()\n",
    "        logging.info(f\"Added document {doc_id} to topic: {topic_name}\")\n",
    "        return True\n",
    "\n",
    "    def get_document_topics(self, file_path):\n",
    "        \"\"\"Get all topics associated with a document\"\"\"\n",
    "        for doc in self.topics_db[\"documents\"]:\n",
    "            if doc[\"file_path\"] == file_path:\n",
    "                return doc[\"topics\"]\n",
    "        return []\n",
    "\n",
    "    def get_topic_documents(self, topic_name):\n",
    "        \"\"\"Get all documents associated with a topic\"\"\"\n",
    "        if topic_name not in self.topics_db[\"topics\"]:\n",
    "            return []\n",
    "\n",
    "        document_ids = self.topics_db[\"topics\"][topic_name][\"documents\"]\n",
    "        documents = []\n",
    "\n",
    "        for doc_id in document_ids:\n",
    "            for doc in self.topics_db[\"documents\"]:\n",
    "                if doc[\"id\"] == doc_id:\n",
    "                    documents.append(doc)\n",
    "                    break\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def get_topic_hierarchy(self):\n",
    "        \"\"\"Get the full topic hierarchy\"\"\"\n",
    "        hierarchy = {}\n",
    "\n",
    "        # Get all root topics (those without parents)\n",
    "        root_topics = set(self.topics_db[\"topics\"].keys())\n",
    "        for parent, children in self.topics_db[\"hierarchies\"].items():\n",
    "            for child in children:\n",
    "                if child in root_topics:\n",
    "                    root_topics.remove(child)\n",
    "\n",
    "        # Build hierarchy starting from root topics\n",
    "        def build_hierarchy(topic):\n",
    "            result = {\"name\": topic, \"children\": []}\n",
    "\n",
    "            if topic in self.topics_db[\"hierarchies\"]:\n",
    "                for child in self.topics_db[\"hierarchies\"][topic]:\n",
    "                    result[\"children\"].append(build_hierarchy(child))\n",
    "\n",
    "            return result\n",
    "\n",
    "        # Create hierarchy for each root topic\n",
    "        for topic in root_topics:\n",
    "            hierarchy[topic] = build_hierarchy(topic)\n",
    "\n",
    "        return hierarchy\n",
    "\n",
    "    def process_pdf(self, pdf_path, auto_classify=False, auto_create_topics=False):\n",
    "        \"\"\"\n",
    "        Process a PDF document - extract text, classify, and optionally auto-assign topics\n",
    "\n",
    "        Parameters:\n",
    "        - pdf_path: Path to the PDF file\n",
    "        - auto_classify: If True, automatically assign to similar existing topics\n",
    "        - auto_create_topics: If True, automatically create new topics from NMF results\n",
    "\n",
    "        Returns:\n",
    "        - Dictionary with classification results and actions taken\n",
    "        \"\"\"\n",
    "        # Generate a document ID\n",
    "        doc_id = f\"doc_{uuid.uuid4().hex[:10]}\"\n",
    "\n",
    "        # Classify the document\n",
    "        result = self.classify_document(pdf_path)\n",
    "        if \"error\" in result:\n",
    "            return result\n",
    "\n",
    "        actions_taken = []\n",
    "\n",
    "        # Auto-assign to existing topics if specified\n",
    "        if auto_classify and result[\"suggested_topics\"]:\n",
    "            for topic_suggestion in result[\"suggested_topics\"]:\n",
    "                topic_name = topic_suggestion[\"name\"]\n",
    "                if (\n",
    "                    topic_suggestion[\"similarity\"] > 0.2\n",
    "                ):  # Minimum threshold for auto-assignment\n",
    "                    self.add_document_to_topic(\n",
    "                        doc_id, topic_name, pdf_path, result[\"keywords\"]\n",
    "                    )\n",
    "                    actions_taken.append(\n",
    "                        f\"Auto-assigned to existing topic: {topic_name}\"\n",
    "                    )\n",
    "\n",
    "        # Auto-create new topics if specified\n",
    "        if auto_create_topics and result[\"potential_topics\"]:\n",
    "            # Use the first NMF topic as a new topic\n",
    "            for potential_topic in result[\"potential_topics\"]:\n",
    "                if potential_topic[\"source\"] == \"NMF\":\n",
    "                    # Create a topic name from the keywords\n",
    "                    topic_name = \"_\".join(potential_topic[\"keywords\"][:2])\n",
    "                    topic_name = re.sub(r\"\\W+\", \"_\", topic_name)\n",
    "\n",
    "                    # Only create if it doesn't exist\n",
    "                    if topic_name not in self.topics_db[\"topics\"]:\n",
    "                        self.add_topic(topic_name, potential_topic[\"keywords\"])\n",
    "                        self.add_document_to_topic(\n",
    "                            doc_id, topic_name, pdf_path, result[\"keywords\"]\n",
    "                        )\n",
    "                        actions_taken.append(\n",
    "                            f\"Auto-created and assigned to new topic: {topic_name}\"\n",
    "                        )\n",
    "                    break\n",
    "\n",
    "        # Add actions taken to the result\n",
    "        result[\"doc_id\"] = doc_id\n",
    "        result[\"actions_taken\"] = actions_taken\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def process_directory(\n",
    "    directory_path, classifier=None, auto_classify=False, auto_create_topics=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Process all PDF files in a directory\n",
    "\n",
    "    Parameters:\n",
    "    - directory_path: Path to directory containing PDFs\n",
    "    - classifier: Optional existing classifier instance\n",
    "    - auto_classify: Whether to automatically assign to similar topics\n",
    "    - auto_create_topics: Whether to automatically create new topics\n",
    "\n",
    "    Returns:\n",
    "    - List of results for each processed file\n",
    "    \"\"\"\n",
    "    if classifier is None:\n",
    "        classifier = DynamicTopicClassifier()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Get all PDF files in the directory\n",
    "    pdf_files = list(Path(directory_path).glob(\"*.pdf\"))\n",
    "    logging.info(f\"Found {len(pdf_files)} PDF files in {directory_path}\")\n",
    "\n",
    "    # Process each file\n",
    "    for pdf_path in pdf_files:\n",
    "        result = classifier.process_pdf(\n",
    "            str(pdf_path),\n",
    "            auto_classify=auto_classify,\n",
    "            auto_create_topics=auto_create_topics,\n",
    "        )\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_classification_summary(results):\n",
    "    \"\"\"Print a summary of classification results\"\"\"\n",
    "    print(\"\\n===== Classification Summary =====\")\n",
    "\n",
    "    for result in results:\n",
    "        if \"error\" in result:\n",
    "            print(\n",
    "                f\"Error processing {result.get('file', 'unknown file')}: {result['error']}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nFile: {result['file']}\")\n",
    "        print(f\"Top Keywords: {', '.join(result['keywords'][:5])}\")\n",
    "\n",
    "        if result[\"suggested_topics\"]:\n",
    "            print(\"Suggested Topics:\")\n",
    "            for topic in result[\"suggested_topics\"][:3]:\n",
    "                print(f\"  - {topic['name']} (similarity: {topic['similarity']:.2f})\")\n",
    "\n",
    "        if result[\"actions_taken\"]:\n",
    "            print(\"Actions Taken:\")\n",
    "            for action in result[\"actions_taken\"]:\n",
    "                print(f\"  - {action}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Create the classifier\n",
    "    classifier = DynamicTopicClassifier()\n",
    "\n",
    "    # Example usage: Process a single PDF\n",
    "    pdf_path = \"sample_document.pdf\"  # Change this to your actual PDF path\n",
    "    if os.path.exists(pdf_path):\n",
    "        print(f\"Processing single document: {pdf_path}\")\n",
    "        result = classifier.process_pdf(pdf_path)\n",
    "\n",
    "        if \"error\" not in result:\n",
    "            print(f\"\\nDocument: {result['file']}\")\n",
    "            print(f\"Keywords: {', '.join(result['keywords'])}\")\n",
    "\n",
    "            if result[\"suggested_topics\"]:\n",
    "                print(\"\\nSuggested Topics:\")\n",
    "                for topic in result[\"suggested_topics\"]:\n",
    "                    print(f\"- {topic['name']} (similarity: {topic['similarity']:.2f})\")\n",
    "                    print(f\"  Keywords: {', '.join(topic['keywords'])}\")\n",
    "\n",
    "            print(\"\\nPotential New Topics:\")\n",
    "            for topic in result[\"potential_topics\"][:3]:\n",
    "                print(f\"- {topic['name']} ({topic['source']})\")\n",
    "                print(f\"  Keywords: {', '.join(topic['keywords'])}\")\n",
    "        else:\n",
    "            print(f\"Error: {result['error']}\")\n",
    "\n",
    "    # Example usage: Process a directory of PDFs\n",
    "    directory_path = \"/Users/bharaths/Developer/score/\"  # Change this to your actual directory path\n",
    "    if os.path.exists(directory_path):\n",
    "        print(f\"\\nProcessing directory: {directory_path}\")\n",
    "        results = process_directory(\n",
    "            directory_path,\n",
    "            classifier=classifier,\n",
    "            auto_classify=True,  # Automatically assign to similar topics\n",
    "            auto_create_topics=True,  # Automatically create new topics\n",
    "        )\n",
    "\n",
    "        # Print summary\n",
    "        print_classification_summary(results)\n",
    "\n",
    "        # Print topic hierarchy\n",
    "        hierarchy = classifier.get_topic_hierarchy()\n",
    "        print(\"\\n===== Topic Hierarchy =====\")\n",
    "\n",
    "        def print_hierarchy(hierarchy_item, level=0):\n",
    "            print(f\"{'  ' * level}└─ {hierarchy_item['name']}\")\n",
    "            for child in hierarchy_item[\"children\"]:\n",
    "                print_hierarchy(child, level + 1)\n",
    "\n",
    "        for topic, hierarchy_item in hierarchy.items():\n",
    "            print_hierarchy(hierarchy_item)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing PDF: No text extracted from PDF\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "from transformers import pipeline\n",
    "from typing import List, Tuple\n",
    "\n",
    "# 1. PDF Text Extraction\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text content from a PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "            with open(pdf_path, \"rb\") as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                for page in reader.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \" \"\n",
    "    except Exception as e:\n",
    "            logging.error(f\"Error extracting text from PDF {pdf_path}: {str(e)}\")\n",
    "    return text\n",
    "# 2. Text Preprocessing\n",
    "def preprocess_text(text: str, max_length: int = 1024) -> str:\n",
    "    \"\"\"\n",
    "    Truncate text to model's maximum length while preserving paragraphs\n",
    "    \"\"\"\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    processed_text = \"\"\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        if len(processed_text) + len(para) < max_length:\n",
    "            processed_text += para + '\\n\\n'\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return processed_text.strip()\n",
    "\n",
    "# 3. Classification Setup\n",
    "class TopicClassifier:\n",
    "    def __init__(self):\n",
    "        self.classifier = pipeline(\n",
    "            \"zero-shot-classification\",\n",
    "            model=\"facebook/bart-large-mnli\"\n",
    "        )\n",
    "        self.taxonomy = {\n",
    "            \"LR Parsing\": \"Compiler Design\",\n",
    "            \"Compiler Design\": \"Computer Science\",\n",
    "            \"Shift-Reduce Parsing\": \"Compiler Design\",\n",
    "            \"Syntax Analysis\": \"Compiler Design\",\n",
    "            \"Computer Science\": None,\n",
    "            \"Biology\": None,\n",
    "            \"Genetics\": \"Biology\"\n",
    "        }\n",
    "    \n",
    "    def get_all_labels(self) -> List[str]:\n",
    "        \"\"\"Get all available labels from taxonomy\"\"\"\n",
    "        return list(self.taxonomy.keys()) + list(set(self.taxonomy.values())) \n",
    "\n",
    "    def classify(self, text: str, threshold: float = 0.7) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Classify text with hierarchical awareness\"\"\"\n",
    "        candidate_labels = self.get_all_labels()\n",
    "        result = self.classifier(text, candidate_labels, multi_label=True)\n",
    "        return [(label, score) for label, score in zip(result['labels'], result['scores']) \n",
    "                if score >= threshold]\n",
    "\n",
    "    def expand_hierarchy(self, labels: List[str]) -> List[str]:\n",
    "        \"\"\"Add parent tags for hierarchical taxonomy\"\"\"\n",
    "        all_tags = set()\n",
    "        for label in labels:\n",
    "            current_tag = label\n",
    "            while current_tag is not None:\n",
    "                all_tags.add(current_tag)\n",
    "                current_tag = self.taxonomy.get(current_tag)\n",
    "        return sorted(all_tags, key=lambda x: (len(self.taxonomy.get(x, '') or 0, x)))\n",
    "\n",
    "# 4. Main Workflow\n",
    "def process_pdf(pdf_path: str, threshold: float = 0.7) -> List[str]:\n",
    "    # Extract text\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Preprocess\n",
    "    processed_text = preprocess_text(raw_text)\n",
    "    \n",
    "    if not processed_text:\n",
    "        raise ValueError(\"No text extracted from PDF\")\n",
    "    \n",
    "    # Classify\n",
    "    classifier = TopicClassifier()\n",
    "    classifications = classifier.classify(processed_text, threshold)\n",
    "    \n",
    "    # Get base labels\n",
    "    base_labels = [label for label, _ in classifications]\n",
    "    \n",
    "    # Expand hierarchy\n",
    "    full_tags = classifier.expand_hierarchy(base_labels)\n",
    "    \n",
    "    return full_tags\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    pdf_path = \"/Users/bharaths/Developer/score/temp_Module 1.pdf\"  # Replace with your PDF path\n",
    "    confidence_threshold = 0.7\n",
    "    \n",
    "    try:\n",
    "        tags = process_pdf(pdf_path, confidence_threshold)\n",
    "        print(\"Automatically Generated Tags:\")\n",
    "        print(\"\\n\".join(f\"- {tag}\" for tag in tags))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
